[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Traffic Assignment Validation",
    "section": "",
    "text": "This document validates the Travel Demand Modelâ€™s traffic assignment by comparing modeled volumes against observed counts from Continuous Count Stations (CCS).\nThe validation process involves: 1. Data Preparation: Aggregating modeled directional flows into bi-directional segment totals. 2. Spatial Linking: Matching CCS stations to Model Segments based on Route and Milepost. 3. Enrichment: Attributing matches with Functional Class, Area Type, and County. 4. Interactive Analysis: Visualizing the differences using OJS."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Traffic Assignment Validation",
    "section": "",
    "text": "This document validates the Travel Demand Modelâ€™s traffic assignment by comparing modeled volumes against observed counts from Continuous Count Stations (CCS).\nThe validation process involves: 1. Data Preparation: Aggregating modeled directional flows into bi-directional segment totals. 2. Spatial Linking: Matching CCS stations to Model Segments based on Route and Milepost. 3. Enrichment: Attributing matches with Functional Class, Area Type, and County. 4. Interactive Analysis: Visualizing the differences using OJS."
  },
  {
    "objectID": "index.html#environment-setup",
    "href": "index.html#environment-setup",
    "title": "Traffic Assignment Validation",
    "section": "0.2 Environment Setup",
    "text": "0.2 Environment Setup\nWe begin by importing the necessary Python libraries for geospatial data handling and setting up the helper functions.\n\nImport Standard Libraries\n\n\nShow the code\n# For Analysis\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\n# For Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport mapclassify\n\n# misc\nimport os\nimport requests\nfrom pathlib import Path\nimport importlib.util\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nTrue"
  },
  {
    "objectID": "index.html#environment-variables",
    "href": "index.html#environment-variables",
    "title": "Traffic Assignment Validation",
    "section": "0.3 Environment Variables",
    "text": "0.3 Environment Variables\n\n\nShow the code\nCRS_UTM = \"EPSG:26912\""
  },
  {
    "objectID": "index.html#helper-functions",
    "href": "index.html#helper-functions",
    "title": "Traffic Assignment Validation",
    "section": "0.4 Helper Functions",
    "text": "0.4 Helper Functions\n\n\nShow the code\ndef fetch_github(\n    url: str,\n    mode: str = \"private\",\n    token_env_var: str = \"GITHUB_TOKEN\"\n) -&gt; requests.Response:\n    \"\"\"\n    Fetch content from GitHub repositories.\n\n    Args:\n        url: GitHub raw URL (e.g., https://raw.githubusercontent.com/...)\n        mode: \"public\" for public repos, \"private\" for private repos requiring authentication\n        token_env_var: Name of environment variable containing GitHub token (default: GITHUB_TOKEN)\n\n    Returns:\n        requests.Response object\n\n    Raises:\n        ValueError: If token is missing for private mode or invalid mode\n        requests.HTTPError: If request fails\n    \"\"\"\n    # Validate mode\n    if mode not in [\"public\", \"private\"]:\n        raise ValueError(f\"mode must be 'public' or 'private', got '{mode}'\")\n\n    if mode == \"public\":\n        response = requests.get(url, timeout=30)\n    else:\n        token = os.getenv(token_env_var)\n        if not token:\n            raise ValueError(\n                f\"GitHub token not found in environment variable '{token_env_var}'. \"\n                f\"Check your .env file has: {token_env_var}=your_token_here\"\n            )\n\n        headers = {\n            'Authorization': f'token {token}',\n            'Accept': 'application/vnd.github.v3.raw'\n        }\n        response = requests.get(url, headers=headers, timeout=30)\n\n    response.raise_for_status()\n    return response\n\n\n\n\nShow the code\n# Create function to read ArcGIS FeatureLayer or Table\ndef arc_read(url, where=\"1=1\", outFields=\"*\", outSR=4326, **kwargs):\n    \"\"\"\n    Read an ArcGIS FeatureLayer or Table to a GeoDataFrame.\n\n    Parameters:\n    url (str): The ArcGIS REST service URL (e.g., ending in /FeatureServer/0)\n    where (str): SQL WHERE clause for filtering. Default: \"1=1\"\n    outFields (str): Comma-separated field names. Default: \"*\"\n    outSR (int): Output spatial reference EPSG code. Default: 4326\n    **kwargs: Additional query parameters passed to the ArcGIS REST API\n\n    Returns:\n    geopandas.GeoDataFrame: Spatial data from the service\n    \"\"\"\n    # Ensure URL ends with /query\n    if not url.endswith('/query'):\n        url = url.rstrip('/') + '/query'\n\n    # Build query parameters\n    params = {\n        'where': where,\n        'outFields': outFields,\n        'returnGeometry': 'true',\n        'outSR': outSR,\n        'f': 'geojson'\n    }\n\n    # Add any additional parameters\n    params.update(kwargs)\n\n    # Make request\n    response = requests.get(url, params=params)\n    response.raise_for_status() # Good practice to check for HTTP errors\n\n    # Read as GeoDataFrame\n    # We use io.BytesIO to handle the response content safely for read_file\n    import io\n    return gpd.read_file(io.BytesIO(response.content), engine=\"pyogrio\")"
  },
  {
    "objectID": "index.html#setup-bigquery",
    "href": "index.html#setup-bigquery",
    "title": "Traffic Assignment Validation",
    "section": "0.5 Setup BigQuery",
    "text": "0.5 Setup BigQuery\n\n\nShow the code\n# # Import global TDM functions from remote 'Resource' repo\n\n# Fetch and import BigQuery module\nresponse = fetch_github(\n    'https://raw.githubusercontent.com/WFRCAnalytics/Resources/refs/heads/master/2-Python/global-functions/BigQuery.py',\n    mode=\"public\"\n)\n\nBigQuery = importlib.util.module_from_spec(importlib.util.spec_from_loader('BigQuery', loader=None))\nexec(response.text, BigQuery.__dict__)\n\n# Initiatlize BigQuery Client\nclient = BigQuery.getBigQueryClient_Confidential2023UtahHTS()\n\n\npukar.bhandari\nC:/Users/Pukar.Bhandari/.private/confidential-2023-utah-hts-5fd7ddd219a7.json"
  },
  {
    "objectID": "index.html#modeled-traffic-volumes",
    "href": "index.html#modeled-traffic-volumes",
    "title": "Traffic Assignment Validation",
    "section": "1.1 Modeled Traffic Volumes",
    "text": "1.1 Modeled Traffic Volumes\nSource: WF-TDM-Development\\Scenarios\\v1000-calib\\BY_2023\\5_AssignHwy\\4_Summaries\\Summary_SEGID_Detailed.csv\n\n\nShow the code\ndf_segid_detail = pd.read_csv(Path(\"_data/raw/Summary_SEGID_Detailed.csv\"))\n\ndf_segid_detail[\"SEGID\"].nunique()\n\n\n4365"
  },
  {
    "objectID": "index.html#model-segments",
    "href": "index.html#model-segments",
    "title": "Traffic Assignment Validation",
    "section": "1.2 Model Segments",
    "text": "1.2 Model Segments\nHere we filter out links that has SEGID corresponding to modeled volume data.\nSegment SHP Source: WF-TDM-Official-Releases\\1_Inputs\\6_Segment\n\n\nShow the code\n# 1. Get the list of unique SEGIDs and reate the SQL query\nvalid_segids = df_segid_detail[\"SEGID\"].dropna().unique().tolist()\nsegid_str = \", \".join(map(lambda x: repr(str(x)), valid_segids))\n\n# 3. Load the segments\ngdf_segments = gpd.read_file(\n    Path(\"_data/raw/Segments/WFv910_Segments.shp\"),\n    where=f\"SEGID IN ({segid_str})\",\n    engine=\"pyogrio\",\n).to_crs(CRS_UTM)\n\nlen(gdf_segments)\n\n\n4364\n\n\n\n\nShow the code\n# One less SEGID in segmets shapefile\ngdf_segments[\"SEGID\"].nunique() == df_segid_detail[\"SEGID\"].nunique()\n\n\nFalse"
  },
  {
    "objectID": "index.html#prepare-ccs-data",
    "href": "index.html#prepare-ccs-data",
    "title": "Traffic Assignment Validation",
    "section": "2.1 Prepare CCS Data",
    "text": "2.1 Prepare CCS Data\n\n\nShow the code\ngdf_obs_counts = gdf_ccs_locations.merge(\n    df_ccs_2023, on=\"STATION\", how=\"inner\"\n)\n\ngdf_obs_counts.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "index.html#prepare-model-data",
    "href": "index.html#prepare-model-data",
    "title": "Traffic Assignment Validation",
    "section": "2.2 Prepare Model Data",
    "text": "2.2 Prepare Model Data\n\nFilter for Aggregate Totals\n\n\nShow the code\n# We use 'FUNCGROUP' = 'Total' (All Lanes) and 'DIRECTION' = 'Both' (Bi-Directional)\ndf_model_clean = df_segid_detail[\n    (df_segid_detail['FUNCGROUP'] == 'Total') &\n    (df_segid_detail['DIRECTION'] == 'Both')\n].copy()\n\n\n\n\nParse SEGID for Route and Milepost (Format: XXXX_YYY.Y)\n\n\nShow the code\n# We handle non-numeric prefixes (like 'MAG' or 'WFRC') by coercing to NaN\ntemp_route = df_model_clean['SEGID'].str.split('_').str[0]\ndf_model_clean.loc[:, 'MODEL_ROUTE'] = pd.to_numeric(\n    temp_route, errors='coerce')\n\n# Parse SEGID & Routes\ndf_model_clean = df_model_clean.dropna(subset=['MODEL_ROUTE']).copy()\ndf_model_clean['MODEL_ROUTE'] = df_model_clean['MODEL_ROUTE'].astype(int)\ndf_model_clean['MODEL_MP'] = df_model_clean['SEGID'].str.split(\n    '_').str[1].astype(float)\n\n\n\n\nJoin with Segment Geometry\n\n\nShow the code\n# We merge the prepared volumes with the clean gdf_segments geometry\ngdf_model_final = gdf_segments[['SEGID', 'SUTRK2022', 'CUTRK2022', 'CO_FIPS', 'geometry']].merge(\n    df_model_clean[['SEGID', 'MODEL_ROUTE', 'MODEL_MP', 'FTCLASS', 'ATYPENAME',\n                    'AM_Vol', 'MD_Vol', 'PM_Vol', 'EV_Vol', 'DY_Vol']],\n    on='SEGID',\n    how='inner'\n)\n\n\n\n\nAdd County Name using CO_FIPS\n\n\nShow the code\nfips_map = {3: \"Box Elder\", 11: \"Davis\", 35: \"Salt Lake\", 49: \"Utah\", 57: \"Weber\"}\n\ngdf_model_final[\"COUNTY_NAME\"] = (\n    gdf_model_final[\"CO_FIPS\"].map(fips_map).fillna(\"Other\")\n)\n\ngdf_model_final[\"COUNTY_NAME\"].value_counts()\n\n\nCOUNTY_NAME\nSalt Lake    1647\nUtah         1229\nWeber         488\nDavis         440\nBox Elder      91\nName: count, dtype: int64"
  },
  {
    "objectID": "index.html#link-route-milepost",
    "href": "index.html#link-route-milepost",
    "title": "Traffic Assignment Validation",
    "section": "3.1 Link Route & Milepost",
    "text": "3.1 Link Route & Milepost\n\n\nShow the code\nmatch_results = []\nused_segids = set()\n\n# --- PASS 1: High-Confidence Route & MP Matching ---\nprint(\"Starting Pass 1: Route & Milepost Matching...\")\n\nfor idx, ccs_row in gdf_obs_counts.iterrows():\n    station_id = ccs_row[\"STATION\"]\n    target_route = ccs_row[\"ROUTE\"]\n    target_mp = ccs_row[\"MP\"]\n\n    # Filter Model Data to the specific Route\n    route_segments = gdf_model_final[gdf_model_final[\"MODEL_ROUTE\"] == target_route]\n\n    matched_segid = None\n    match_type = \"No Match\"\n\n    if not route_segments.empty:\n        # Priority A: Predecessor (Segment BMP &lt;= Station MP)\n        predecessors = route_segments[route_segments[\"MODEL_MP\"] &lt;= target_mp]\n        if not predecessors.empty:\n            best_match = predecessors.loc[predecessors[\"MODEL_MP\"].idxmax()]\n            matched_segid = best_match[\"SEGID\"]\n            match_type = \"Predecessor (&lt;=)\"\n        else:\n            # Priority B: Successor (Segment BMP &gt; Station MP)\n            successors = route_segments[route_segments[\"MODEL_MP\"] &gt; target_mp]\n            if not successors.empty:\n                best_match = successors.loc[successors[\"MODEL_MP\"].idxmin()]\n                matched_segid = best_match[\"SEGID\"]\n                match_type = \"Successor (&gt;)\"\n\n    # If we found a match, add it to our \"Used\" set immediately\n    if matched_segid:\n        used_segids.add(matched_segid)\n\n    match_results.append(\n        {\n            \"STATION\": station_id,\n            \"MATCHED_SEGID\": matched_segid,\n            \"MATCH_TYPE\": match_type,\n            \"geometry\": ccs_row[\"geometry\"],  # Keep geometry for Pass 2\n        }\n    )\n\n# --- PASS 2: Spatial Fallback for Unmatched Stations ---\nprint(\"Starting Pass 2: Spatial Fallback (excluding used segments)...\")\n\nfor i, result in enumerate(match_results):\n    if result[\"MATCHED_SEGID\"] is None:\n        station_geom = result[\"geometry\"]\n\n        # Filter available segments: Must NOT be in the \"Used\" set\n        # This prevents taking a segment that was matched by Route/MP\n        available_segments = gdf_model_final[\n            ~gdf_model_final[\"SEGID\"].isin(used_segids)\n        ]\n\n        if not available_segments.empty:\n            # Calculate distance to all AVAILABLE segments\n            # (Note: Warning - Ensure CRS matches. Assuming both are NAD83/UTM12N or similar projected CRS)\n            distances = available_segments.distance(station_geom)\n\n            # Find closest\n            nearest_idx = distances.idxmin()\n            best_match = available_segments.loc[nearest_idx]\n\n            # Assign Match\n            matched_segid = best_match[\"SEGID\"]\n            match_results[i][\"MATCHED_SEGID\"] = matched_segid\n            match_results[i][\"MATCH_TYPE\"] = \"Spatial Fallback\"\n\n            # Mark this SEGID as used so the next fallback doesn't grab it\n            used_segids.add(matched_segid)\n\n# Convert to DataFrame\ndf_matches = pd.DataFrame(match_results).drop(columns=[\"geometry\"])\nprint(\"Final Match Breakdown:\")\nprint(df_matches[\"MATCH_TYPE\"].value_counts())\n\n\nStarting Pass 1: Route & Milepost Matching...\nStarting Pass 2: Spatial Fallback (excluding used segments)...\nFinal Match Breakdown:\nMATCH_TYPE\nPredecessor (&lt;=)    81\nSpatial Fallback     2\nName: count, dtype: int64"
  },
  {
    "objectID": "index.html#create-master-validation-table",
    "href": "index.html#create-master-validation-table",
    "title": "Traffic Assignment Validation",
    "section": "3.2 Create Master Validation Table",
    "text": "3.2 Create Master Validation Table\n\nJoin Observed Counts and Model Attributes\n\n\nShow the code\n# 1. Join Matches to Observed Counts (Inner Join filters to valid stations only)\ndf_merged = df_matches.merge(df_ccs_2023, on='STATION', how='inner')\n\n# 2. Join to Model Attributes\ndf_merged = df_merged.merge(\n    gdf_model_final[['SEGID', 'FTCLASS', 'ATYPENAME', 'COUNTY_NAME',\n                     'SUTRK2022', 'CUTRK2022',\n                     'AM_Vol', 'MD_Vol', 'PM_Vol', 'EV_Vol', 'DY_Vol']],\n    left_on='MATCHED_SEGID',\n    right_on='SEGID',\n    how='left'\n)\n\n\n\n\nPivot Periods (Wide -&gt; Long)\n\n\nShow the code\n# 3. Pivot to Long Format (The \"Tidy\" Data Step)\n# We melt the dataframe so \"Period\" becomes a dimension, not a column name.\n\n# A. Melt Observed Counts\nid_vars = [\n    \"STATION\",\n    \"MATCHED_SEGID\",\n    \"FTCLASS\",\n    \"ATYPENAME\",\n    \"COUNTY_NAME\",\n    \"SUTRK2022\",\n    \"CUTRK2022\",\n]\n\ndf_obs_long = df_merged.melt(\n    id_vars=id_vars,\n    value_vars=[\"AVG_AM_VOL\", \"AVG_MD_VOL\", \"AVG_PM_VOL\", \"AVG_EV_VOL\"],\n    var_name=\"PERIOD_KEY\",\n    value_name=\"OBSERVED_TOTAL\",\n)\n\n# Extract simple period name (AM, MD, PM, EV, DY) from the column name\ndf_obs_long[\"PERIOD\"] = df_obs_long[\"PERIOD_KEY\"].str.split(\"_\").str[1]\n\n\n\n\nConvert Modeled Data to Long Format\n\n\nShow the code\n# B. Melt Modeled Counts\n# We use the same ID variables to ensure alignment\ndf_mod_long = df_merged.melt(\n    id_vars=id_vars,\n    value_vars=[\"AM_Vol\", \"MD_Vol\", \"PM_Vol\", \"EV_Vol\"],\n    var_name=\"MOD_KEY\",\n    value_name=\"MODELED_TOTAL\",\n)\n\n# Extract period name (AM, MD...) - splitting 'AM_Vol' gives 'AM' at index 0\ndf_mod_long[\"PERIOD\"] = df_mod_long[\"MOD_KEY\"].str.split(\"_\").str[0]\n\n\n\n\nMerge to get Period-level Total Volumes\n\n\nShow the code\n# Merge to get Period-level Total Volumes\ndf_period = pd.merge(\n    df_obs_long.drop(columns=['PERIOD_KEY']),\n    df_mod_long.drop(columns=['MOD_KEY']),\n    on=id_vars + ['PERIOD'],\n    how='inner'\n)\n\n\n\n\nExpand Vehicle Types (Vectorized)\n\n\nShow the code\n# First, ensure all percentages exist\ndf_period[\"PCT_SUT\"] = df_period[\"SUTRK2022\"].fillna(0)\ndf_period[\"PCT_CUT\"] = df_period[\"CUTRK2022\"].fillna(0)\ndf_period[\"PCT_AUTO\"] = 1.0 - (df_period[\"PCT_SUT\"] + df_period[\"PCT_CUT\"])\n\n# Melt the percentages into a single column\ndf_long = df_period.melt(\n    id_vars=[\n        \"STATION\",\n        \"MATCHED_SEGID\",\n        \"FTCLASS\",\n        \"ATYPENAME\",\n        \"COUNTY_NAME\",\n        \"PERIOD\",\n        \"OBSERVED_TOTAL\",\n        \"MODELED_TOTAL\",\n    ],\n    value_vars=[\"PCT_AUTO\", \"PCT_SUT\", \"PCT_CUT\"],\n    var_name=\"TYPE_KEY\",\n    value_name=\"SHARE\",\n)\n\n# Clean up Vehicle labels\ntype_map = {\"PCT_AUTO\": \"Auto\", \"PCT_SUT\": \"SUT\", \"PCT_CUT\": \"CUT\"}\ndf_long[\"VEHICLE_TYPE\"] = df_long[\"TYPE_KEY\"].map(type_map)\n\n# Single vectorized multiplication for all rows\ndf_long[\"OBSERVED\"] = df_long[\"OBSERVED_TOTAL\"] * df_long[\"SHARE\"]\ndf_long[\"MODELED\"] = df_long[\"MODELED_TOTAL\"] * df_long[\"SHARE\"]\n\n\n\n\nFinal Cleanup & Export\n\n\nShow the code\ncols_final = [\n    \"STATION\",\n    \"MATCHED_SEGID\",\n    \"FTCLASS\",\n    \"ATYPENAME\",\n    \"COUNTY_NAME\",\n    \"PERIOD\",\n    \"VEHICLE_TYPE\",\n    \"OBSERVED\",\n    \"MODELED\",\n]\ndf_final = df_long[cols_final].copy()\n\n# Calculate stats\ndf_final[\"DIFF\"] = df_final[\"MODELED\"] - df_final[\"OBSERVED\"]\ndf_final[\"PCT_DIFF\"] = (df_final[\"DIFF\"] / df_final[\"OBSERVED\"]).round(3)\n\ndf_final.to_csv(\"_output/dashboard_data.csv\", index=False)"
  },
  {
    "objectID": "index.html#interactive-dashboard",
    "href": "index.html#interactive-dashboard",
    "title": "Traffic Assignment Validation",
    "section": "3.3 Interactive Dashboard",
    "text": "3.3 Interactive Dashboard\n\n\nShow the code\nimport { aq, op } from '@uwdata/arquero'\nimport { Plot } from '@observablehq/plot'\n\n// 2. LOAD DATA\ndata_raw = FileAttachment(\"_output/dashboard_data.csv\").csv({ typed: true })\n\n// 3. DASHBOARD CONTROLS\nviewof dashboard_ctrls = Inputs.form({\n  group_by: Inputs.select(\n    [\"FTCLASS\", \"ATYPENAME\", \"COUNTY_NAME\", \"PERIOD\", \"VEHICLE_TYPE\"],\n    {label: \"ðŸ“Š Color / Group By:\", value: \"FTCLASS\"}\n  ),\n  filter_county: Inputs.select(\n    [\"All\"].concat(Array.from(new Set(data_raw.map(d =&gt; d.COUNTY_NAME))).sort()),\n    {label: \"Filter County:\", value: \"All\"}\n  ),\n  filter_atype: Inputs.select(\n    [\"All\"].concat(Array.from(new Set(data_raw.map(d =&gt; d.ATYPENAME))).sort()),\n    {label: \"Filter Area Type:\", value: \"All\"}\n  ),\n  filter_period: Inputs.select(\n    [\"All\", \"AM\", \"MD\", \"PM\", \"EV\"],\n    {label: \"Filter Period:\", value: \"All\"}\n  ),\n  filter_vehicle: Inputs.select(\n    [\"All\", \"Auto\", \"SUT\", \"CUT\"],\n    {label: \"Filter Vehicle:\", value: \"All\"}\n  )\n})\n\n// 4. DATA PROCESSING PIPELINE\n// We create a reactive object 'dash_data' that holds both datasets we need\ndash_data = {\n  const { group_by, filter_county, filter_atype, filter_period, filter_vehicle } = dashboard_ctrls;\n\n  // A. Base Filtered Data\n  // We pass the filter values as parameters ($) to avoid scope errors\n  let base = aq.from(data_raw)\n    .params({\n      fc: filter_county, fa: filter_atype, fp: filter_period, fv: filter_vehicle, gb: group_by\n    })\n    .filter((d, $) =&gt; $.fc === \"All\" || d.COUNTY_NAME === $.fc)\n    .filter((d, $) =&gt; $.fa === \"All\" || d.ATYPENAME === $.fa)\n    .filter((d, $) =&gt; $.fp === \"All\" || d.PERIOD === $.fp)\n    .filter((d, $) =&gt; $.fv === \"All\" || d.VEHICLE_TYPE === $.fv)\n\n  // B. Scatter Data (Station Level)\n  // Group by Station AND the \"Group By\" variable to keep the color dimension\n  let scatter = base\n    .groupby(\"STATION\", group_by)\n    .rollup({\n      Obs_Total: op.sum('OBSERVED'),\n      Mod_Total: op.sum('MODELED')\n    })\n    .derive({\n      Label: (d, $) =&gt; d[$.gb], // Dynamic label for coloring\n      Max_Vol: d =&gt; Math.max(d.Obs_Total, d.Mod_Total) // For scaling\n    })\n\n  // C. Bar/Table Data (Aggregate Level)\n  // Group purely by the \"Group By\" variable\n  let summary = base\n    .groupby(group_by)\n    .rollup({\n      Stations: op.count(),\n      Obs_Total: op.sum('OBSERVED'),\n      Mod_Total: op.sum('MODELED')\n    })\n    .derive({\n      Diff: d =&gt; d.Mod_Total - d.Obs_Total,\n      Pct_Diff: d =&gt; (d.Mod_Total - d.Obs_Total) / d.Obs_Total,\n      Label: (d, $) =&gt; d[$.gb]\n    })\n    .orderby(group_by)\n\n  return { scatter, summary }\n}\n\n// 5. PLOT PREPARATION\n// Calculate max value for the reference line (y=x)\nmax_val = dash_data.scatter.rollup({ max: op.max(\"Max_Vol\") }).get(\"max\")\n\n// Fold the summary data to \"Long\" format for the Grouped Bar Chart\nbar_plot_data = dash_data.summary.fold([\"Obs_Total\", \"Mod_Total\"], { as: [\"Metric\", \"Volume\"] })\n    .derive({ Metric_Label: d =&gt; d.Metric === \"Obs_Total\" ? \"Observed\" : \"Modeled\" })\n\n// 6. VISUALIZATION LAYOUT\n\nmd`### 1. Station Scatter Plot`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nmd`Each dot represents a station. Points above the diagonal line are over-predicted; points below are under-predicted.`\n\n\n\n\n\n\n\n\n\nShow the code\nPlot.plot({\n  height: 500,\n  grid: true,\n  aspectRatio: 1,\n  x: { label: \"Observed Volume\", tickFormat: \"s\", domain: [0, max_val] },\n  y: { label: \"Modeled Volume\", tickFormat: \"s\", domain: [0, max_val] },\n  color: { legend: true, label: dashboard_ctrls.group_by },\n  marks: [\n    // Reference Line (y=x)\n    Plot.line([[0, 0], [max_val, max_val]], { stroke: \"#ccc\", strokeDasharray: \"4\" }),\n\n    // Scatter Dots\n    Plot.dot(dash_data.scatter, {\n      x: \"Obs_Total\",\n      y: \"Mod_Total\",\n      fill: \"Label\",\n      stroke: \"white\",\n      strokeWidth: 1,\n      opacity: 0.8,\n      r: 4,\n      title: d =&gt; `Station: ${d.STATION}\\n${dashboard_ctrls.group_by}: ${d.Label}\\nObs: ${d.Obs_Total.toLocaleString()}\\nMod: ${d.Mod_Total.toLocaleString()}`\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\nShow the code\nmd`### 2. Aggregate Comparison`\n\n\n\n\n\n\n\n\n\nShow the code\nmd`Comparison of total volumes by category.`\n\n\n\n\n\n\n\n\n\nShow the code\nPlot.plot({\n  marginLeft: 150,\n  height: Math.max(300, dash_data.summary.numRows() * 50), // Auto-resize height\n  color: { domain: [\"Observed\", \"Modeled\"], range: [\"#bababa\", \"#1976d2\"] },\n  x: { label: \"Total Volume\", grid: true, tickFormat: \"s\" },\n  y: { axis: null },\n  fy: { label: null, axis: \"left\" }, // Facet Y acts as our categorical axis\n  marks: [\n    Plot.barX(bar_plot_data, {\n      x: \"Volume\",\n      y: \"Metric_Label\", // Stacks the two bars side-by-side\n      fy: \"Label\",       // Groups them by category\n      fill: \"Metric_Label\",\n      title: d =&gt; `${d.Metric_Label}: ${d.Volume.toLocaleString()}`,\n      sort: { fy: \"x\", reduce: \"max\", order: \"descending\" }\n    }),\n    Plot.text(bar_plot_data, {\n      x: \"Volume\", y: \"Metric_Label\", fy: \"Label\",\n      text: d =&gt; d.Volume.toLocaleString(),\n      dx: 5, textAnchor: \"start\", fill: \"#666\", fontSize: 10\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\nShow the code\nmd`### 3. Statistics Table`\n\n\n\n\n\n\n\n\n\nShow the code\nInputs.table(dash_data.summary, {\n  columns: [\"Label\", \"Stations\", \"Obs_Total\", \"Mod_Total\", \"Diff\", \"Pct_Diff\"],\n  header: {\n    Label: dashboard_ctrls.group_by,\n    Obs_Total: \"Observed\",\n    Mod_Total: \"Modeled\",\n    Diff: \"Diff\",\n    Pct_Diff: \"% Error\"\n  },\n  format: {\n    Obs_Total: d =&gt; d.toLocaleString(),\n    Mod_Total: d =&gt; d.toLocaleString(),\n    Diff: d =&gt; d.toLocaleString(),\n    Pct_Diff: d =&gt; (d * 100).toFixed(1) + \"%\"\n  }\n})"
  }
]