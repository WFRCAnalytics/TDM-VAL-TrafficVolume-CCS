---
title: Traffic Assignment Validation
subtitle: Validating the Obsvered vs Modeled Traffic Volume
description: This notebook is used to validate the traffic volume assigned by the Travel Demand Model against the observed traffic volume data captured by the Continuous Counting Stations (CCS).
author:
 - name: Pukar Bhandari
   email: pukar.bhandari@wfrc.utah.gov
   affiliation:
     - name: Wasatch Front Regional Council
       url: "https://wfrc.utah.gov/"
date: "2025-12-24"
---

## Overview

This document validates the Travel Demand Model's traffic assignment by comparing modeled volumes against observed counts from Continuous Count Stations (CCS).

The validation process involves:
1. **Data Preparation:** Aggregating modeled directional flows into bi-directional segment totals.
2. **Spatial Linking:** Matching CCS stations to Model Segments based on Route and Milepost.
3. **Enrichment:** Attributing matches with Functional Class, Area Type, and County.
4. **Interactive Analysis:** Visualizing the differences using OJS.

## Environment Setup

We begin by importing the necessary Python libraries for geospatial data handling and setting up the helper functions.

### Import Standard Libraries

```{python}
# For Analysis
import numpy as np
import pandas as pd
import geopandas as gpd

# For Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import mapclassify

# misc
import os
import requests
from pathlib import Path
import importlib.util

from dotenv import load_dotenv

load_dotenv()
```

## Environment Variables

```{python}
CRS_UTM = "EPSG:26912"
```

## Helper Functions

```{python}
def fetch_github(
    url: str,
    mode: str = "private",
    token_env_var: str = "GITHUB_TOKEN"
) -> requests.Response:
    """
    Fetch content from GitHub repositories.

    Args:
        url: GitHub raw URL (e.g., https://raw.githubusercontent.com/...)
        mode: "public" for public repos, "private" for private repos requiring authentication
        token_env_var: Name of environment variable containing GitHub token (default: GITHUB_TOKEN)

    Returns:
        requests.Response object

    Raises:
        ValueError: If token is missing for private mode or invalid mode
        requests.HTTPError: If request fails
    """
    # Validate mode
    if mode not in ["public", "private"]:
        raise ValueError(f"mode must be 'public' or 'private', got '{mode}'")

    if mode == "public":
        response = requests.get(url, timeout=30)
    else:
        token = os.getenv(token_env_var)
        if not token:
            raise ValueError(
                f"GitHub token not found in environment variable '{token_env_var}'. "
                f"Check your .env file has: {token_env_var}=your_token_here"
            )

        headers = {
            'Authorization': f'token {token}',
            'Accept': 'application/vnd.github.v3.raw'
        }
        response = requests.get(url, headers=headers, timeout=30)

    response.raise_for_status()
    return response
```

```{python}
# Create function to read ArcGIS FeatureLayer or Table
def arc_read(url, where="1=1", outFields="*", outSR=4326, **kwargs):
    """
    Read an ArcGIS FeatureLayer or Table to a GeoDataFrame.

    Parameters:
    url (str): The ArcGIS REST service URL (e.g., ending in /FeatureServer/0)
    where (str): SQL WHERE clause for filtering. Default: "1=1"
    outFields (str): Comma-separated field names. Default: "*"
    outSR (int): Output spatial reference EPSG code. Default: 4326
    **kwargs: Additional query parameters passed to the ArcGIS REST API

    Returns:
    geopandas.GeoDataFrame: Spatial data from the service
    """
    # Ensure URL ends with /query
    if not url.endswith('/query'):
        url = url.rstrip('/') + '/query'

    # Build query parameters
    params = {
        'where': where,
        'outFields': outFields,
        'returnGeometry': 'true',
        'outSR': outSR,
        'f': 'geojson'
    }

    # Add any additional parameters
    params.update(kwargs)

    # Make request
    response = requests.get(url, params=params)
    response.raise_for_status() # Good practice to check for HTTP errors

    # Read as GeoDataFrame
    # We use io.BytesIO to handle the response content safely for read_file
    import io
    return gpd.read_file(io.BytesIO(response.content), engine="pyogrio")
```

## Setup BigQuery

```{python}
# # Import global TDM functions from remote 'Resource' repo

# Fetch and import BigQuery module
response = fetch_github(
    'https://raw.githubusercontent.com/WFRCAnalytics/Resources/refs/heads/master/2-Python/global-functions/BigQuery.py',
    mode="public"
)

BigQuery = importlib.util.module_from_spec(importlib.util.spec_from_loader('BigQuery', loader=None))
exec(response.text, BigQuery.__dict__)

# Initiatlize BigQuery Client
client = BigQuery.getBigQueryClient_Confidential2023UtahHTS()
```

# Load Data

### Regional Boundary

```{python}
# 1. Configuration
service_url = "https://services1.arcgis.com/taguadKoI1XFwivx/ArcGIS/rest/services/RegionalBoundaryComponents/FeatureServer/0"

output_dir = Path("_data/raw")
gdb_path = output_dir / "RegionalBoundary.gpkg"
layer_name = "RegionalBoundaryComponents"

# 2. Check if file exists locally, otherwise download
if not gdb_path.exists():
    print(f"File not found at {gdb_path}. Downloading Regional Boundaries...")
    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        # Use the custom arc_read function
        gdf_download = arc_read(service_url)

        # Export to Geodatabase
        print(f"Saving to {gdb_path}...")
        gdf_download.to_file(gdb_path, layer=layer_name, driver="GPKG")
        print("Download and export complete.")

    except Exception as e:
        print(f"Error during download: {e}")
else:
    print(f"File found locally at {gdb_path}. Skipping download.")

# 3. Read from local GDB with Filter and Dissolve
gdf_region = gpd.read_file(
    gdb_path,
    layer=layer_name,
    where="PlanOrg IN ('MAG MPO', 'WFRC MPO')"
).dissolve().to_crs(CRS_UTM)

# Fix the holes and slivers in the regional boundary
gdf_region['geometry'] = gdf_region.buffer(1).buffer(-1)

gdf_region.explore()
```

### CCS Station

```{python}
# 1. Setup Configuration
service_url = "https://services.arcgis.com/pA2nEVnB6tquxgOW/ArcGIS/rest/services/CCS_Data_v2/FeatureServer/1"
output_dir = Path("_data/raw")
gdb_path = output_dir / "CCS_Data_v2.gpkg"
layer_name = "CCS_Data_v2"

# 2. Check if file exists locally
if not gdb_path.exists():
    print(f"File not found at {gdb_path}. Downloading from ArcGIS Feature Service...")

    # Ensure directory exists
    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        # --- Use custom arc_read function ---
        gdf_download = arc_read(service_url)

        # --- Cast Integer64 columns to Int32 to avoid Float conversion ---
        if "SITEID" in gdf_download.columns:
            gdf_download["SITEID"] = gdf_download["SITEID"].astype("int32")

        # Export to Geodatabase
        print(f"Saving to {gdb_path}...")
        gdf_download.to_file(gdb_path, layer="CCS_Data_v2", driver="GPKG")
        print("Download and export complete.")

    except Exception as e:
        print(f"Error during download or export: {e}")
else:
    print(f"File found locally at {gdb_path}. Skipping download.")

# 3. Load the data back into Python
# We load from the local GDB (cache) and apply the County filter here
gdf_ccs_locations = gpd.read_file(
    gdb_path,
    layer="CCS_Data_v2",
    # where="COUNTY IN ('Box Elder', 'Weber', 'Davis', 'Salt Lake', 'Utah')",
    mask=gdf_region,
).to_crs(CRS_UTM)

gdf_ccs_locations.explore()
```

```{python}
# Check the number of unique count stations
gdf_ccs_locations["STATION"].nunique()
```

### UDOT CCS Traffic Counts

```{python}
# Extract unique stations and format them as a SQL list string
# This creates a string like: "'301', '302', '405'"
station_ids = gdf_ccs_locations["STATION"].unique()
sql_station_list = ", ".join([str(x) for x in station_ids])
```

```{python}
# Define query
query = f"""
    SELECT
        STATION,
        CAST(ROUND(AVG(AM_SUM)) AS INT64) as AVG_AM_VOL,
        CAST(ROUND(AVG(MD_SUM)) AS INT64) as AVG_MD_VOL,
        CAST(ROUND(AVG(PM_SUM)) AS INT64) as AVG_PM_VOL,
        CAST(ROUND(AVG(EV_SUM)) AS INT64) as AVG_EV_VOL,
        CAST(ROUND(AVG(DAILY_SUM)) AS INT64) as AVG_DY_VOL
    FROM (
        SELECT
            STATION,
            DATE_ONLY,
            SUM(CASE WHEN HOUR BETWEEN 6 AND 8 THEN VOLUME ELSE 0 END) as AM_SUM,
            SUM(CASE WHEN HOUR BETWEEN 9 AND 14 THEN VOLUME ELSE 0 END) as MD_SUM,
            SUM(CASE WHEN HOUR BETWEEN 15 AND 17 THEN VOLUME ELSE 0 END) as PM_SUM,
            SUM(CASE WHEN HOUR >= 18 OR HOUR <= 5 THEN VOLUME ELSE 0 END) as EV_SUM,
            SUM(VOLUME) as DAILY_SUM
        FROM `wfrc-modeling-data.src_udot_continuous_count_2023.data`
        WHERE
            DATE_ONLY BETWEEN '2023-09-01' AND '2023-11-15'
            AND EXTRACT(DAYOFWEEK FROM DATE_ONLY) IN (3, 4, 5)
            -- FILTER: Only include stations found in our local GeoDataFrame
            -- AND STATION IN ({{sql_station_list}})
        GROUP BY
            STATION, DATE_ONLY
    )
    GROUP BY
        STATION
    ORDER BY
        STATION
"""

# Load Linked Trips data from UDOT's 023 Continuos Counter Station Data
df_ccs_2023 = client.query(query).to_dataframe()

df_ccs_2023
```

Find Route and Begin Mile Post from SEGID (RRRR_PPP.P) in Model data (Model Network and Volumes), use Route to join and take Milepost equal or less than that of the CCS Station.

```{python}
# # Load Linked Trips data from UDOT's 023 Continuos Counter Station Data
# df_ccs_2023 = client.query(
#     "SELECT * FROM `wfrc-modeling-data.src_udot_continuous_count_2023.data`"
# ).to_dataframe()

# df_ccs_2023
```

```{python}
df_ccs_2023["STATION"].nunique()
```

```{python}
print(
    {
        "Location Only": len(
            set(gdf_ccs_locations["STATION"]) - set(df_ccs_2023["STATION"])
        ),
        "Both": len(set(gdf_ccs_locations["STATION"]) & set(df_ccs_2023["STATION"])),
        "Count Only": len(
            set(df_ccs_2023["STATION"]) - set(gdf_ccs_locations["STATION"])
        ),
    }
)
```

```{python}
# View the stations that have no count data in 2023
gdf_ccs_locations[
    ~gdf_ccs_locations["STATION"].isin(df_ccs_2023["STATION"].unique())
].explore()
```

```{python}
# Check the number of stations where sum of periodic volume is not equal to daily volume
(
    df_ccs_2023[["AVG_AM_VOL", "AVG_MD_VOL", "AVG_PM_VOL", "AVG_EV_VOL"]].sum(axis=1)
    != df_ccs_2023["AVG_DY_VOL"]
).sum()
```

```{python}
# Total volume difference across all stations
(
    df_ccs_2023["AVG_AM_VOL"].sum()
    + df_ccs_2023["AVG_MD_VOL"].sum()
    + df_ccs_2023["AVG_PM_VOL"].sum()
    + df_ccs_2023["AVG_EV_VOL"].sum()
) - df_ccs_2023["AVG_DY_VOL"].sum()
```

## Modeled Traffic Volumes

Source: `WF-TDM-Development\Scenarios\v1000-calib\BY_2023\5_AssignHwy\4_Summaries`

```{python}
df_segid_summary = pd.read_csv(
    Path("_data/raw/Summary_SEGID.csv")
)

df_segid_summary["SEGID"].nunique()
```

```{python}
df_segid_detail = pd.read_csv(Path("_data/raw/Summary_SEGID_Detailed.csv"))

df_segid_detail["SEGID"].nunique()
```

## Model Network

Here we filter out links that has `SEGID` corresponding to modeled volume data.

Network SHP Source: `WF-TDM-Official-Releases\1_Inputs\3_Highway\GIS`
Segment SHP Source: `WF-TDM-Official-Releases\1_Inputs\6_Segment`

```{python}
# 1. Get the list of unique SEGIDs and reate the SQL query
valid_segids = df_segid_summary["SEGID"].dropna().unique().tolist()
segid_str = ", ".join(map(lambda x: repr(str(x)), valid_segids))

# 3. Load the network
gdf_network_v10 = gpd.read_file(
    Path("_data/raw/MasterNet/WFv1000_MasterNet_20250821 - Link.shp"),
    where=f"SEGID IN ({segid_str})",
    engine="pyogrio",
).to_crs(CRS_UTM)

len(gdf_network_v10)
```

Verify if the number of unique ID in traffic volume data and network data are the same.

```{python}
gdf_network_v10['SEGID'].nunique() == df_segid_summary['SEGID'].nunique()
```

Additionally, let's confirm that multiple links have same `SEGID`.

```{python}
# This will show you which SEGIDs have the most geographic pieces
counts = gdf_network_v10['SEGID'].value_counts()
print(counts.head(10))

# To see the average number of links per segment:
print(f"Average links per SEGID: {len(gdf_network_v10) / df_segid_summary['SEGID'].nunique():.2f}")
```

## Model Segments

```{python}
# 1. Get the list of unique SEGIDs and reate the SQL query
valid_segids = df_segid_summary["SEGID"].dropna().unique().tolist()
segid_str = ", ".join(map(lambda x: repr(str(x)), valid_segids))

# 3. Load the segments
gdf_segments_v10 = gpd.read_file(
    Path("_data/raw/Segments/WFv910_Segments.shp"),
    where=f"SEGID IN ({segid_str})",
    engine="pyogrio",
).to_crs(CRS_UTM)

len(gdf_segments_v10)
```

```{python}
# One less SEGID in segmets shapefile
gdf_segments_v10["SEGID"].nunique() == df_segid_summary["SEGID"].nunique()
```

# Data Cleaning

## Merge Traffic Counts and Station

```{python}
gdf_traffic_counts = gdf_ccs_locations.merge(df_ccs_2023, on="STATION", how="inner")
gdf_traffic_counts.explore()
```

## Merge Segment Volume and Model Network

```{python}
gdf_network_dissolved = (
    # 1. Prepare Network Data for Dissolve
    # We keep SEGID (key), geometry (for map), and STREET (for tooltips/validation)
    # Most other attributes (FT, CO_FIPS, Volumes) are already in df_segid_summary
    gdf_network_v10[['SEGID', 'STREET', 'geometry']].copy()
    # 2. Dissolve geometries by SEGID
    # This aggregates the multiple links per segment into a single MultiLineString
    .dissolve(
        by='SEGID',
        aggfunc={'STREET': 'first'}, # Keep the first street name found
        as_index=False
    )
)

# 3. Merge with Modeled Volume Data
# We merge 'inner' to ensure we have both geometry and model data for all rows
gdf_model_segments = gdf_network_dissolved.merge(
    df_segid_summary,
    on='SEGID',
    how='inner'
)

gdf_model_segments
```

```{python}
gdf_model_segments.explore(
    column="DY_VMT",
    scheme="NaturalBreaks",  # Divides data into 5 natural clusters
    k=5,
    cmap="RdYlGn_r",
    tiles="CartoDB voyager",
    style_kwds={"weight": 2.5},  # Base weight
)
```

## Merge Segment Volume and Model Segments

```{python}
gdf_segment_volume = gdf_segments_v10[["SEGID", "SUBAREAID", "geometry"]].merge(
    df_segid_summary,
    on=["SEGID", "SUBAREAID"],
    # on=["SEGID", "DISTANCE", "DIRECTION", "CO_FIPS", "SUBAREAID"],
    how="inner",
)
```

```{python}
gdf_segment_volume.explore(
    column="DY_VMT",
    scheme="NaturalBreaks",  # Divides data into 5 natural clusters
    k=5,
    cmap="RdYlGn_r",
    tiles="CartoDB voyager",
    style_kwds={"weight": 2.5},  # Base weight
)
```

# Spatial Operations

## Model Network


```{python}
# | label: prep-validation-data
# | echo: true
# | warning: false

# --- 1. Prepare Model Data (The "Total" Volume View) ---
# Filter for 'Total' (All Lanes) and 'Both' (Bi-Directional)
df_model_clean = df_segid_detail[
    (df_segid_detail['FUNCGROUP'] == 'Total') &
    (df_segid_detail['DIRECTION'] == 'Both')
].copy()

# Robust Route Extraction (Handle 'MAG', 'WFRC' prefixes by coercing to NaN)
temp_route = df_model_clean['SEGID'].str.split('_').str[0]
df_model_clean['MODEL_ROUTE'] = pd.to_numeric(temp_route, errors='coerce')

# Drop invalid routes and cast to Integer
df_model_clean = df_model_clean.dropna(subset=['MODEL_ROUTE'])
df_model_clean['MODEL_ROUTE'] = df_model_clean['MODEL_ROUTE'].astype(int)

# Extract Milepost
df_model_clean['MODEL_MP'] = df_model_clean['SEGID'].str.split(
    '_').str[1].astype(float)

# Select columns - NOW INCLUDING CO_FIPS (County)
cols = ['SEGID', 'MODEL_ROUTE', 'MODEL_MP', 'FTCLASS', 'ATYPENAME', 'CO_FIPS',
        'AM_Vol', 'MD_Vol', 'PM_Vol', 'EV_Vol', 'DY_Vol']
df_model_final = df_model_clean[cols].reset_index(drop=True)

# Map FIPS to County Names for readability (Standard Utah WFRC/MAG codes)
fips_map = {3: 'Box Elder', 11: 'Davis',
            35: 'Salt Lake', 49: 'Utah', 57: 'Weber'}
df_model_final['COUNTY_NAME'] = df_model_final['CO_FIPS'].map(
    fips_map).fillna('Other')

# --- 2. Prepare CCS Data ---
gdf_ccs_process = gdf_ccs_locations.copy()

# Clean Route (Force to Integer)
gdf_ccs_process['CCS_ROUTE'] = pd.to_numeric(
    gdf_ccs_process['ROUTE'], errors='coerce')
gdf_ccs_process = gdf_ccs_process.dropna(subset=['CCS_ROUTE'])
gdf_ccs_process['CCS_ROUTE'] = gdf_ccs_process['CCS_ROUTE'].astype(int)

# --- 3. Iterative Matching Logic (Route + MP) ---
match_results = []

for idx, ccs_row in gdf_ccs_process.iterrows():
    station_id = ccs_row['STATION']
    target_route = ccs_row['CCS_ROUTE']
    target_mp = ccs_row['MP']

    # Filter Model Data to same Route
    route_segments = df_model_final[df_model_final['MODEL_ROUTE']
                                    == target_route]

    matched_segid = None
    match_type = "No Match"

    if not route_segments.empty:
        # Priority A: Predecessor (Segment Starts Before Station)
        predecessors = route_segments[route_segments['MODEL_MP'] <= target_mp]

        if not predecessors.empty:
            # Pick the closest one (Max MP)
            best_match = predecessors.loc[predecessors['MODEL_MP'].idxmax()]
            matched_segid = best_match['SEGID']
            match_type = "Predecessor (<=)"
        else:
            # Priority B: Successor (Segment Starts After Station)
            successors = route_segments[route_segments['MODEL_MP'] > target_mp]

            if not successors.empty:
                # Pick the closest one (Min MP)
                best_match = successors.loc[successors['MODEL_MP'].idxmin()]
                matched_segid = best_match['SEGID']
                match_type = "Successor (>)"

    match_results.append({
        'STATION': station_id,
        'MATCHED_SEGID': matched_segid,
        'MATCH_TYPE': match_type
    })

df_matches = pd.DataFrame(match_results)

# --- 4. Final Merge (Create Master Validation Table) ---
# A. Join Matches to Observed Counts (Inner Join)
df_validation = df_matches.merge(df_ccs_2023, on='STATION', how='inner')

# B. Join to Model Attributes (Using the Matched SEGID)
# We pull FTCLASS, ATYPENAME, and COUNTY from here
df_validation = df_validation.merge(
    df_model_final[['SEGID', 'FTCLASS', 'ATYPENAME', 'COUNTY_NAME',
                    'AM_Vol', 'MD_Vol', 'PM_Vol', 'EV_Vol', 'DY_Vol']],
    left_on='MATCHED_SEGID',
    right_on='SEGID',
    how='left'
)

# Calculate Daily Diff for quick check
df_validation['DIFF_DY'] = df_validation['DY_Vol'] - \
    df_validation['AVG_DY_VOL']

# --- 5. Export to OJS ---
# We convert to a list of dictionaries for easy consumption in JavaScript
ojs_data = df_validation.to_dict(orient='records')
```

```{python}
# | label: transfer-to-ojs
# | echo: false

from IPython.display import display, HTML
import json

# Standard Quarto way to pass data to OJS
print("Transferring data to OJS...")
# ojs_define is a Quarto-specific function available in .qmd files
ojs_define(data_py=ojs_data)
```

```{ojs}
//| label: ojs-dashboard
//| echo: false

// 1. Import libraries
import { aq, op } from '@uwdata/arquero'
import { Plot } from '@observablehq/plot'

// 2. Create the Dropdown Input
viewof group_var = Inputs.select(
  ["FTCLASS", "ATYPENAME", "COUNTY_NAME"],
  {label: "Group Validation By:", value: "FTCLASS"}
)

// 3. Process Data (Arquero is great for this in browser)
// We pivot the data to sum Observed and Modeled volumes by the selected group
summary_data = {
  // Convert Python data to Arquero Table
  let dt = aq.from(data_py)
    .filter(d => d[group_var] != null) // Remove unmatched rows
    .groupby(group_var)
    .rollup({
      Stations: op.count(),
      Obs_Daily: op.sum('AVG_DY_VOL'),
      Mod_Daily: op.sum('DY_Vol'),
      Obs_AM: op.sum('AVG_AM_VOL'),
      Mod_AM: op.sum('AM_Vol'),
      Obs_PM: op.sum('AVG_PM_VOL'),
      Mod_PM: op.sum('PM_Vol')
    })
    .derive({
      Diff_Daily: d => d.Mod_Daily - d.Obs_Daily,
      Pct_Diff_Daily: d => (d.Mod_Daily - d.Obs_Daily) / d.Obs_Daily,
      // Create a clean label for the plot
      Group: d => d[group_var]
    })
    .orderby(group_var)

  return dt
}

// 4. Render the Summary Table
Inputs.table(summary_data, {
  columns: [
    group_var, "Stations",
    "Obs_Daily", "Mod_Daily", "Diff_Daily", "Pct_Diff_Daily"
  ],
  header: {
    [group_var]: "Category",
    Obs_Daily: "Observed (Daily)",
    Mod_Daily: "Modeled (Daily)",
    Diff_Daily: "Difference",
    Pct_Diff_Daily: "% Difference"
  },
  format: {
    Obs_Daily: d => d.toLocaleString(),
    Mod_Daily: d => d.toLocaleString(),
    Diff_Daily: d => d.toLocaleString(),
    Pct_Diff_Daily: d => (d * 100).toFixed(1) + "%"
  }
})

// 5. Render Interactive Plot (Modeled vs Observed)
Plot.plot({
  title: `Modeled vs Observed Volume by ${group_var}`,
  marginLeft: 150,
  marks: [
    Plot.barX(summary_data, {
      x: "Obs_Daily",
      y: "Group",
      fill: "#e0e0e0", // Light gray for Observed
      title: "Observed"
    }),
    Plot.tickX(summary_data, {
      x: "Mod_Daily",
      y: "Group",
      stroke: d => d.Mod_Daily > d.Obs_Daily ? "red" : "blue", // Red if over-modeled
      strokeWidth: 3,
      title: "Modeled"
    }),
    Plot.text(summary_data, {
      x: d => Math.max(d.Obs_Daily, d.Mod_Daily),
      y: "Group",
      text: d => (d.Pct_Diff_Daily * 100).toFixed(1) + "%",
      dx: 5,
      textAnchor: "start"
    })
  ],
  x: { label: "Daily Traffic Volume" },
  y: { label: "" },
  color: { legend: true }
})
```
