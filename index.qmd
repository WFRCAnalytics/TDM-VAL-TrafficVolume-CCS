---
title: Observed vs Modeled Volume
subtitle: Validating the Modeled Traffic Volume
description: This notebook is used to validate the traffic volume assigned by the Travel Demand Model against the observed traffic volume data captured by the Continuous Counting Stations (CCS).
author:
 - name: Pukar Bhandari
   email: pukar.bhandari@wfrc.utah.gov
   affiliation:
     - name: Wasatch Front Regional Council
       url: "https://wfrc.utah.gov/"
date: "2025-12-24"
---

## Environment Setup

### Import Standard Libraries

```{python}
# For Analysis
import numpy as np
import pandas as pd
import geopandas as gpd

# For Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# misc
import os
import requests
from pathlib import Path
import importlib.util

from dotenv import load_dotenv
load_dotenv()
```

## Environment Variables

```{python}
CRS_UTM = "EPSG:26912"
```

## Helper Functions

```{python}
def fetch_github(
    url: str,
    mode: str = "private",
    token_env_var: str = "GITHUB_TOKEN"
) -> requests.Response:
    """
    Fetch content from GitHub repositories.

    Args:
        url: GitHub raw URL (e.g., https://raw.githubusercontent.com/...)
        mode: "public" for public repos, "private" for private repos requiring authentication
        token_env_var: Name of environment variable containing GitHub token (default: GITHUB_TOKEN)

    Returns:
        requests.Response object

    Raises:
        ValueError: If token is missing for private mode or invalid mode
        requests.HTTPError: If request fails
    """
    # Validate mode
    if mode not in ["public", "private"]:
        raise ValueError(f"mode must be 'public' or 'private', got '{mode}'")

    if mode == "public":
        response = requests.get(url, timeout=30)
    else:
        token = os.getenv(token_env_var)
        if not token:
            raise ValueError(
                f"GitHub token not found in environment variable '{token_env_var}'. "
                f"Check your .env file has: {token_env_var}=your_token_here"
            )

        headers = {
            'Authorization': f'token {token}',
            'Accept': 'application/vnd.github.v3.raw'
        }
        response = requests.get(url, headers=headers, timeout=30)

    response.raise_for_status()
    return response
```

```{python}
# Create function to read ArcGIS FeatureLayer or Table
def arc_read(url, where="1=1", outFields="*", outSR=4326, **kwargs):
    """
    Read an ArcGIS FeatureLayer or Table to a GeoDataFrame.

    Parameters:
    url (str): The ArcGIS REST service URL (e.g., ending in /FeatureServer/0)
    where (str): SQL WHERE clause for filtering. Default: "1=1"
    outFields (str): Comma-separated field names. Default: "*"
    outSR (int): Output spatial reference EPSG code. Default: 4326
    **kwargs: Additional query parameters passed to the ArcGIS REST API

    Returns:
    geopandas.GeoDataFrame: Spatial data from the service
    """
    # Ensure URL ends with /query
    if not url.endswith('/query'):
        url = url.rstrip('/') + '/query'

    # Build query parameters
    params = {
        'where': where,
        'outFields': outFields,
        'returnGeometry': 'true',
        'outSR': outSR,
        'f': 'geojson'
    }

    # Add any additional parameters
    params.update(kwargs)

    # Make request
    response = requests.get(url, params=params)
    response.raise_for_status() # Good practice to check for HTTP errors

    # Read as GeoDataFrame
    # We use io.BytesIO to handle the response content safely for read_file
    import io
    return gpd.read_file(io.BytesIO(response.content), engine="pyogrio")
```

## Setup BigQuery

```{python}
# # Import global TDM functions from remote 'Resource' repo

# Fetch and import BigQuery module
response = fetch_github(
    'https://raw.githubusercontent.com/WFRCAnalytics/Resources/refs/heads/master/2-Python/global-functions/BigQuery.py',
    mode="public"
)

BigQuery = importlib.util.module_from_spec(importlib.util.spec_from_loader('BigQuery', loader=None))
exec(response.text, BigQuery.__dict__)

# Initiatlize BigQuery Client
client = BigQuery.getBigQueryClient_Confidential2023UtahHTS()
```

# Load Data

### Regional Boundary

```{python}
# 1. Configuration
service_url = "https://services1.arcgis.com/taguadKoI1XFwivx/ArcGIS/rest/services/RegionalBoundaryComponents/FeatureServer/0"

output_dir = Path("_data/raw")
gdb_path = output_dir / "RegionalBoundary.gpkg"
layer_name = "RegionalBoundaryComponents"

# 2. Check if file exists locally, otherwise download
if not gdb_path.exists():
    print(f"File not found at {gdb_path}. Downloading Regional Boundaries...")
    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        # Use the custom arc_read function
        gdf_download = arc_read(service_url)

        # Export to Geodatabase
        print(f"Saving to {gdb_path}...")
        gdf_download.to_file(gdb_path, layer=layer_name, driver="GPKG")
        print("Download and export complete.")

    except Exception as e:
        print(f"Error during download: {e}")
else:
    print(f"File found locally at {gdb_path}. Skipping download.")

# 3. Read from local GDB with Filter and Dissolve
gdf_region = gpd.read_file(
    gdb_path,
    layer=layer_name,
    where="PlanOrg IN ('MAG MPO', 'WFRC MPO')"
).dissolve().to_crs(CRS_UTM)

# Fix the holes and slivers in the regional boundary
gdf_region['geometry'] = gdf_region.buffer(1).buffer(-1)

gdf_region.explore()
```

### UDOT CCS Traffic Counts

```{python}
# Load Linked Trips data from UDOT's 023 Continuos Counter Station Data
df_ccs_2023 = client.query(
    "SELECT * FROM " + 'wfrc-modeling-data.src_udot_continuous_count_2023.data'
).to_dataframe()

df_ccs_2023
```

```{python}
# Check the number of unique count stations
df_ccs_2023["STATION"].nunique()
```

### CCS Station

```{python}
# 1. Setup Configuration
service_url = "https://services.arcgis.com/pA2nEVnB6tquxgOW/ArcGIS/rest/services/CCS_Data_v2/FeatureServer/1"
output_dir = Path("_data/raw")
gdb_path = output_dir / "CCS_Data_v2.gpkg"
layer_name = "CCS_Data_v2"

# 2. Check if file exists locally
if not gdb_path.exists():
    print(f"File not found at {gdb_path}. Downloading from ArcGIS Feature Service...")

    # Ensure directory exists
    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        # --- Use custom arc_read function ---
        gdf_download = arc_read(service_url)

        # --- Cast Integer64 columns to Int32 to avoid Float conversion ---
        if 'SITEID' in gdf_download.columns:
            gdf_download['SITEID'] = gdf_download['SITEID'].astype('int32')

        # Export to Geodatabase
        print(f"Saving to {gdb_path}...")
        gdf_download.to_file(gdb_path, layer="CCS_Data_v2", driver="GPKG")
        print("Download and export complete.")

    except Exception as e:
        print(f"Error during download or export: {e}")
else:
    print(f"File found locally at {gdb_path}. Skipping download.")

# 3. Load the data back into Python
# We load from the local GDB (cache) and apply the County filter here
gdf_ccs_locations = (
    gpd.read_file(
        gdb_path,
        layer="CCS_Data_v2",
        where="COUNTY IN ('Box Elder', 'Weber', 'Davis', 'Salt Lake', 'Utah')"
    )
    .to_crs(CRS_UTM)
    .pipe(gpd.clip, mask=gdf_region)
)

gdf_ccs_locations.explore()
```

```{python}
# Check the number of unique count stations
gdf_ccs_locations["STATION"].nunique()
```

```{python}
# View the stations that have no count data in 2023
gdf_ccs_locations[
    ~gdf_ccs_locations["STATION"].isin(df_ccs_2023["STATION"].unique())
].explore()
```

## Modeled Traffic Volumes

```{python}
df_model_volume = pd.read_csv(
    Path("_data/raw/Summary_SEGID.csv")
)

df_model_volume
```

## Model Network

Here we filter out links that has `SEGID` corresponding to modeled volume data.

```{python}
# 1. Get the list of unique SEGIDs from your model volume dataframe
# We filter out any NAs just to be safe
valid_segids = df_model_volume['SEGID'].dropna().unique().tolist()

# 2. Create the SQL query
# We force conversion to string using str(x), then use repr() to wrap it in quotes.
# Example: 0006_152.6 -> '0006_152.6' (with quotes for SQL)
segid_str = ', '.join(map(lambda x: repr(str(x)), valid_segids))

# 3. Load the network
gdf_network_v10 = gpd.read_file(
    Path("_data/raw/WFv1000_MasterNet/WFv1000_MasterNet_20250821 - Link.shp"),
    where=f"SEGID IN ({segid_str})",
    engine="pyogrio"
).to_crs(CRS_UTM)

len(gdf_network_v10)
```

Verify if the number of unique ID in traffic volume data and network data are the same.

```{python}
gdf_network_v10['SEGID'].nunique() == df_model_volume['SEGID'].nunique()
```

Additionally, let's confirm that multiple links have same `SEGID`.

```{python}
# This will show you which SEGIDs have the most geographic pieces
counts = gdf_network_v10['SEGID'].value_counts()
print(counts.head(10))

# To see the average number of links per segment:
print(f"Average links per SEGID: {len(gdf_network_v10) / df_model_volume['SEGID'].nunique():.2f}")
```

# Data Cleaning

## Merge Traffic Counts and Station

```{python}
gdf_traffic_counts = gdf_ccs_locations.merge(
    df_ccs_2023,
    on="STATION",
    how="inner"
)
gdf_traffic_counts
```

## Merge Modeled Volume and Network

```{python}
gdf_network_dissolved = (
    # 1. Prepare Network Data for Dissolve
    # We keep SEGID (key), geometry (for map), and STREET (for tooltips/validation)
    # Most other attributes (FT, CO_FIPS, Volumes) are already in df_model_volume
    gdf_network_v10[['SEGID', 'STREET', 'geometry']].copy()
    # 2. Dissolve geometries by SEGID
    # This aggregates the multiple links per segment into a single MultiLineString
    .dissolve(
        by='SEGID',
        aggfunc={'STREET': 'first'}, # Keep the first street name found
        as_index=False
    )
)

# 3. Merge with Modeled Volume Data
# We merge 'inner' to ensure we have both geometry and model data for all rows
gdf_model_segments = gdf_network_dissolved.merge(
    df_model_volume,
    on='SEGID',
    how='inner'
)

gdf_model_segments
```

```{python}
gdf_model_segments.explore()
```

# Spatial Operations

```{python}
# 1. Find the nearest network link for each CCS Station
# We use the CCS stations as the 'left' dataframe because we want to iterate
# through every station and find its single closest neighbor.
gdf_ccs_mapped = gpd.sjoin_nearest(
    gdf_ccs_locations,
    gdf_model_segments[['SEGID', 'STREET', 'geometry']], # Only keep keys/geometry to keep it clean
    how="left",          # Keep all stations
    distance_col="distance_meters" # Optional: records the distance to the match
)

# 2. Quality Control (Optional but Recommended)
# View the results sorted by distance (Descending)
# This puts the most "suspicious" matches (furthest away) at the top
gdf_ccs_mapped.sort_values("distance_meters", ascending=False)[
    ['STATION', 'SEGID', 'STREET', 'distance_meters']
]
```

```{python}
# Merge model links with station geometry
gdf_links_with_dist = gdf_model_segments.merge(
    gdf_ccs_locations[['SEGID', 'geometry']],
    on='SEGID',
    how='left',
    suffixes=('', '_station')
)

# Calculate distance between link geometry and station geometry
gdf_links_with_dist['distance_meters'] = gdf_links_with_dist.geometry.distance(
    gpd.GeoSeries(gdf_links_with_dist['geometry_station'], crs=CRS_UTM)
)

# Clean up (Optional: remove the extra geometry column if you want)
gdf_links_with_dist.drop(columns=['geometry_station'], inplace=True)
```
