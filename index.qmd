---
title: Traffic Assignment Validation
subtitle: Validating the Obsvered vs Modeled Traffic Volume
description: This notebook is used to validate the traffic volume assigned by the Travel Demand Model against the observed traffic volume data captured by the Continuous Counting Stations (CCS).
author:
 - name: Pukar Bhandari
   email: pukar.bhandari@wfrc.utah.gov
   affiliation:
     - name: Wasatch Front Regional Council
       url: "https://wfrc.utah.gov/"
date: "2025-12-24"
---

## Overview

This document validates the Travel Demand Model's traffic assignment by comparing modeled volumes against observed counts from Continuous Count Stations (CCS).

The validation process involves:
1. **Data Preparation:** Aggregating modeled directional flows into bi-directional segment totals.
2. **Spatial Linking:** Matching CCS stations to Model Segments based on Route and Milepost.
3. **Enrichment:** Attributing matches with Functional Class, Area Type, and County.
4. **Interactive Analysis:** Visualizing the differences using OJS.

## Environment Setup

We begin by importing the necessary Python libraries for geospatial data handling and setting up the helper functions.

### Import Standard Libraries

```{python}
# For Analysis
import numpy as np
import pandas as pd
import geopandas as gpd

# For Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import mapclassify

# misc
import os
import requests
from pathlib import Path
import importlib.util

from dotenv import load_dotenv

load_dotenv()
```

## Environment Variables

```{python}
CRS_UTM = "EPSG:26912"
```

## Helper Functions

```{python}
def fetch_github(
    url: str,
    mode: str = "private",
    token_env_var: str = "GITHUB_TOKEN"
) -> requests.Response:
    """
    Fetch content from GitHub repositories.

    Args:
        url: GitHub raw URL (e.g., https://raw.githubusercontent.com/...)
        mode: "public" for public repos, "private" for private repos requiring authentication
        token_env_var: Name of environment variable containing GitHub token (default: GITHUB_TOKEN)

    Returns:
        requests.Response object

    Raises:
        ValueError: If token is missing for private mode or invalid mode
        requests.HTTPError: If request fails
    """
    # Validate mode
    if mode not in ["public", "private"]:
        raise ValueError(f"mode must be 'public' or 'private', got '{mode}'")

    if mode == "public":
        response = requests.get(url, timeout=30)
    else:
        token = os.getenv(token_env_var)
        if not token:
            raise ValueError(
                f"GitHub token not found in environment variable '{token_env_var}'. "
                f"Check your .env file has: {token_env_var}=your_token_here"
            )

        headers = {
            'Authorization': f'token {token}',
            'Accept': 'application/vnd.github.v3.raw'
        }
        response = requests.get(url, headers=headers, timeout=30)

    response.raise_for_status()
    return response
```

```{python}
# Create function to read ArcGIS FeatureLayer or Table
def arc_read(url, where="1=1", outFields="*", outSR=4326, **kwargs):
    """
    Read an ArcGIS FeatureLayer or Table to a GeoDataFrame.

    Parameters:
    url (str): The ArcGIS REST service URL (e.g., ending in /FeatureServer/0)
    where (str): SQL WHERE clause for filtering. Default: "1=1"
    outFields (str): Comma-separated field names. Default: "*"
    outSR (int): Output spatial reference EPSG code. Default: 4326
    **kwargs: Additional query parameters passed to the ArcGIS REST API

    Returns:
    geopandas.GeoDataFrame: Spatial data from the service
    """
    # Ensure URL ends with /query
    if not url.endswith('/query'):
        url = url.rstrip('/') + '/query'

    # Build query parameters
    params = {
        'where': where,
        'outFields': outFields,
        'returnGeometry': 'true',
        'outSR': outSR,
        'f': 'geojson'
    }

    # Add any additional parameters
    params.update(kwargs)

    # Make request
    response = requests.get(url, params=params)
    response.raise_for_status() # Good practice to check for HTTP errors

    # Read as GeoDataFrame
    # We use io.BytesIO to handle the response content safely for read_file
    import io
    return gpd.read_file(io.BytesIO(response.content), engine="pyogrio")
```

## Setup BigQuery

```{python}
# # Import global TDM functions from remote 'Resource' repo

# Fetch and import BigQuery module
response = fetch_github(
    'https://raw.githubusercontent.com/WFRCAnalytics/Resources/refs/heads/master/2-Python/global-functions/BigQuery.py',
    mode="public"
)

BigQuery = importlib.util.module_from_spec(importlib.util.spec_from_loader('BigQuery', loader=None))
exec(response.text, BigQuery.__dict__)

# Initiatlize BigQuery Client
client = BigQuery.getBigQueryClient_Confidential2023UtahHTS()
```

# Load Data

### Regional Boundary

```{python}
# 1. Configuration
service_url = "https://services1.arcgis.com/taguadKoI1XFwivx/ArcGIS/rest/services/RegionalBoundaryComponents/FeatureServer/0"

output_dir = Path("_data/raw")
gdb_path = output_dir / "RegionalBoundary.gpkg"
layer_name = "RegionalBoundaryComponents"

# 2. Check if file exists locally, otherwise download
if not gdb_path.exists():
    print(f"File not found at {gdb_path}. Downloading Regional Boundaries...")
    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        # Use the custom arc_read function
        gdf_download = arc_read(service_url)

        # Export to Geodatabase
        print(f"Saving to {gdb_path}...")
        gdf_download.to_file(gdb_path, layer=layer_name, driver="GPKG")
        print("Download and export complete.")

    except Exception as e:
        print(f"Error during download: {e}")
else:
    print(f"File found locally at {gdb_path}. Skipping download.")

# 3. Read from local GDB with Filter and Dissolve
gdf_region = gpd.read_file(
    gdb_path,
    layer=layer_name,
    where="PlanOrg IN ('MAG MPO', 'WFRC MPO')"
).dissolve().to_crs(CRS_UTM)

# Fix the holes and slivers in the regional boundary
gdf_region['geometry'] = gdf_region.buffer(1).buffer(-1)

gdf_region.explore()
```

### CCS Station

```{python}
# 1. Setup Configuration
service_url = "https://services.arcgis.com/pA2nEVnB6tquxgOW/ArcGIS/rest/services/CCS_Data_v2/FeatureServer/1"
output_dir = Path("_data/raw")
gdb_path = output_dir / "CCS_Data_v2.gpkg"
layer_name = "CCS_Data_v2"

# 2. Check if file exists locally
if not gdb_path.exists():
    print(f"File not found at {gdb_path}. Downloading from ArcGIS Feature Service...")

    # Ensure directory exists
    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        # --- Use custom arc_read function ---
        gdf_download = arc_read(service_url)

        # --- Cast Integer64 columns to Int32 to avoid Float conversion ---
        if "SITEID" in gdf_download.columns:
            gdf_download["SITEID"] = gdf_download["SITEID"].astype("int32")

        # Export to Geodatabase
        print(f"Saving to {gdb_path}...")
        gdf_download.to_file(gdb_path, layer="CCS_Data_v2", driver="GPKG")
        print("Download and export complete.")

    except Exception as e:
        print(f"Error during download or export: {e}")
else:
    print(f"File found locally at {gdb_path}. Skipping download.")

# 3. Load the data back into Python
# We load from the local GDB (cache) and apply the County filter here
gdf_ccs_locations = gpd.read_file(
    gdb_path,
    layer="CCS_Data_v2",
    # where="COUNTY IN ('Box Elder', 'Weber', 'Davis', 'Salt Lake', 'Utah')",
    mask=gdf_region,
).to_crs(CRS_UTM)

gdf_ccs_locations.explore()
```

```{python}
# Check the number of unique count stations
gdf_ccs_locations["STATION"].nunique()
```

### UDOT CCS Traffic Counts

```{python}
# Extract unique stations and format them as a SQL list string
# This creates a string like: "'301', '302', '405'"
station_ids = gdf_ccs_locations["STATION"].unique()
sql_station_list = ", ".join([str(x) for x in station_ids])
```

```{python}
# Define query
query = f"""
    SELECT
        STATION,
        CAST(ROUND(AVG(AM_SUM)) AS INT64) as AVG_AM_VOL,
        CAST(ROUND(AVG(MD_SUM)) AS INT64) as AVG_MD_VOL,
        CAST(ROUND(AVG(PM_SUM)) AS INT64) as AVG_PM_VOL,
        CAST(ROUND(AVG(EV_SUM)) AS INT64) as AVG_EV_VOL,
        CAST(ROUND(AVG(DAILY_SUM)) AS INT64) as AVG_DY_VOL
    FROM (
        SELECT
            STATION,
            DATE_ONLY,
            SUM(CASE WHEN HOUR BETWEEN 6 AND 8 THEN VOLUME ELSE 0 END) as AM_SUM,
            SUM(CASE WHEN HOUR BETWEEN 9 AND 14 THEN VOLUME ELSE 0 END) as MD_SUM,
            SUM(CASE WHEN HOUR BETWEEN 15 AND 17 THEN VOLUME ELSE 0 END) as PM_SUM,
            SUM(CASE WHEN HOUR >= 18 OR HOUR <= 5 THEN VOLUME ELSE 0 END) as EV_SUM,
            SUM(VOLUME) as DAILY_SUM
        FROM `wfrc-modeling-data.src_udot_continuous_count_2023.data`
        WHERE
            DATE_ONLY BETWEEN '2023-09-01' AND '2023-11-15'
            AND EXTRACT(DAYOFWEEK FROM DATE_ONLY) IN (3, 4, 5)
            -- FILTER: Only include stations found in our local GeoDataFrame
            -- AND STATION IN ({{sql_station_list}})
        GROUP BY
            STATION, DATE_ONLY
    )
    GROUP BY
        STATION
    ORDER BY
        STATION
"""

# Load Linked Trips data from UDOT's 023 Continuos Counter Station Data
df_ccs_2023 = client.query(query).to_dataframe()

df_ccs_2023
```

```{python}
#| eval: false
#| include: false

# # Load Linked Trips data from UDOT's 023 Continuos Counter Station Data
# df_ccs_2023 = client.query(
#     "SELECT * FROM `wfrc-modeling-data.src_udot_continuous_count_2023.data`"
# ).to_dataframe()

# df_ccs_2023
```

```{python}
#| eval: false
#| include: false

df_ccs_2023["STATION"].nunique()
```

```{python}
#| eval: false
#| include: false

print(
    {
        "Location Only": len(
            set(gdf_ccs_locations["STATION"]) - set(df_ccs_2023["STATION"])
        ),
        "Both": len(set(gdf_ccs_locations["STATION"]) & set(df_ccs_2023["STATION"])),
        "Count Only": len(
            set(df_ccs_2023["STATION"]) - set(gdf_ccs_locations["STATION"])
        ),
    }
)
```

```{python}
#| eval: false
#| include: false

# View the stations that have no count data in 2023
gdf_ccs_locations[
    ~gdf_ccs_locations["STATION"].isin(df_ccs_2023["STATION"].unique())
].explore()
```

```{python}
#| eval: false
#| include: false

# Check the number of stations where sum of periodic volume is not equal to daily volume
(
    df_ccs_2023[["AVG_AM_VOL", "AVG_MD_VOL", "AVG_PM_VOL", "AVG_EV_VOL"]].sum(axis=1)
    != df_ccs_2023["AVG_DY_VOL"]
).sum()
```

```{python}
#| eval: false
#| include: false

# Total volume difference across all stations
(
    df_ccs_2023["AVG_AM_VOL"].sum()
    + df_ccs_2023["AVG_MD_VOL"].sum()
    + df_ccs_2023["AVG_PM_VOL"].sum()
    + df_ccs_2023["AVG_EV_VOL"].sum()
) - df_ccs_2023["AVG_DY_VOL"].sum()
```

## Modeled Traffic Volumes

Source: `WF-TDM-Development\Scenarios\v1000-calib\BY_2023\5_AssignHwy\4_Summaries\Summary_SEGID_Detailed.csv`

```{python}
df_segid_detail = pd.read_csv(Path("_data/raw/Summary_SEGID_Detailed.csv"))

df_segid_detail["SEGID"].nunique()
```

## Model Segments

Here we filter out links that has `SEGID` corresponding to modeled volume data.

Segment SHP Source: `WF-TDM-Official-Releases\1_Inputs\6_Segment`

```{python}
# 1. Get the list of unique SEGIDs and reate the SQL query
valid_segids = df_segid_detail["SEGID"].dropna().unique().tolist()
segid_str = ", ".join(map(lambda x: repr(str(x)), valid_segids))

# 3. Load the segments
gdf_segments = gpd.read_file(
    Path("_data/raw/Segments/WFv910_Segments.shp"),
    where=f"SEGID IN ({segid_str})",
    engine="pyogrio",
).to_crs(CRS_UTM)

len(gdf_segments)
```

```{python}
# One less SEGID in segmets shapefile
gdf_segments["SEGID"].nunique() == df_segid_detail["SEGID"].nunique()
```

# Data Cleaning

## Prepare CCS Data

```{python}
gdf_obs_counts = gdf_ccs_locations.merge(
    df_ccs_2023, on="STATION", how="inner"
)

gdf_obs_counts.explore()
```

## Prepare Model Data

### Filter for Aggregate Totals

```{python}
# We use 'FUNCGROUP' = 'Total' (All Lanes) and 'DIRECTION' = 'Both' (Bi-Directional)
df_model_clean = df_segid_detail[
    (df_segid_detail['FUNCGROUP'] == 'Total') &
    (df_segid_detail['DIRECTION'] == 'Both')
].copy()
```

### Parse SEGID for Route and Milepost (Format: XXXX_YYY.Y)

```{python}
# We handle non-numeric prefixes (like 'MAG' or 'WFRC') by coercing to NaN
temp_route = df_model_clean['SEGID'].str.split('_').str[0]
df_model_clean.loc[:, 'MODEL_ROUTE'] = pd.to_numeric(
    temp_route, errors='coerce')

# Parse SEGID & Routes
df_model_clean = df_model_clean.dropna(subset=['MODEL_ROUTE']).copy()
df_model_clean['MODEL_ROUTE'] = df_model_clean['MODEL_ROUTE'].astype(int)
df_model_clean['MODEL_MP'] = df_model_clean['SEGID'].str.split(
    '_').str[1].astype(float)
```

### Join with Segment Geometry

```{python}
# We merge the prepared volumes with the clean gdf_segments geometry
gdf_model_final = gdf_segments[['SEGID', 'SUTRK2022', 'CUTRK2022', 'CO_FIPS', 'geometry']].merge(
    df_model_clean[['SEGID', 'MODEL_ROUTE', 'MODEL_MP', 'FTCLASS', 'ATYPENAME',
                    'AM_Vol', 'MD_Vol', 'PM_Vol', 'EV_Vol', 'DY_Vol']],
    on='SEGID',
    how='inner'
)
```

### Add County Name using CO_FIPS

```{python}
fips_map = {3: "Box Elder", 11: "Davis", 35: "Salt Lake", 49: "Utah", 57: "Weber"}

gdf_model_final["COUNTY_NAME"] = (
    gdf_model_final["CO_FIPS"].map(fips_map).fillna("Other")
)

gdf_model_final["COUNTY_NAME"].value_counts()
```

# Spatial Operations

## Link Route & Milepost

```{python}
# | label: link-stations
# | echo: true

match_results = []
used_segids = set()

# --- PASS 1: High-Confidence Route & MP Matching ---
print("Starting Pass 1: Route & Milepost Matching...")

for idx, ccs_row in gdf_obs_counts.iterrows():
    station_id = ccs_row["STATION"]
    target_route = ccs_row["ROUTE"]
    target_mp = ccs_row["MP"]

    # Filter Model Data to the specific Route
    route_segments = gdf_model_final[gdf_model_final["MODEL_ROUTE"] == target_route]

    matched_segid = None
    match_type = "No Match"

    if not route_segments.empty:
        # Priority A: Predecessor (Segment BMP <= Station MP)
        predecessors = route_segments[route_segments["MODEL_MP"] <= target_mp]
        if not predecessors.empty:
            best_match = predecessors.loc[predecessors["MODEL_MP"].idxmax()]
            matched_segid = best_match["SEGID"]
            match_type = "Predecessor (<=)"
        else:
            # Priority B: Successor (Segment BMP > Station MP)
            successors = route_segments[route_segments["MODEL_MP"] > target_mp]
            if not successors.empty:
                best_match = successors.loc[successors["MODEL_MP"].idxmin()]
                matched_segid = best_match["SEGID"]
                match_type = "Successor (>)"

    # If we found a match, add it to our "Used" set immediately
    if matched_segid:
        used_segids.add(matched_segid)

    match_results.append(
        {
            "STATION": station_id,
            "MATCHED_SEGID": matched_segid,
            "MATCH_TYPE": match_type,
            "geometry": ccs_row["geometry"],  # Keep geometry for Pass 2
        }
    )

# --- PASS 2: Spatial Fallback for Unmatched Stations ---
print("Starting Pass 2: Spatial Fallback (excluding used segments)...")

for i, result in enumerate(match_results):
    if result["MATCHED_SEGID"] is None:
        station_geom = result["geometry"]

        # Filter available segments: Must NOT be in the "Used" set
        # This prevents taking a segment that was matched by Route/MP
        available_segments = gdf_model_final[
            ~gdf_model_final["SEGID"].isin(used_segids)
        ]

        if not available_segments.empty:
            # Calculate distance to all AVAILABLE segments
            # (Note: Warning - Ensure CRS matches. Assuming both are NAD83/UTM12N or similar projected CRS)
            distances = available_segments.distance(station_geom)

            # Find closest
            nearest_idx = distances.idxmin()
            best_match = available_segments.loc[nearest_idx]

            # Assign Match
            matched_segid = best_match["SEGID"]
            match_results[i]["MATCHED_SEGID"] = matched_segid
            match_results[i]["MATCH_TYPE"] = "Spatial Fallback"

            # Mark this SEGID as used so the next fallback doesn't grab it
            used_segids.add(matched_segid)

# Convert to DataFrame
df_matches = pd.DataFrame(match_results).drop(columns=["geometry"])
print("Final Match Breakdown:")
print(df_matches["MATCH_TYPE"].value_counts())
```

## Create Master Validation Table

### Join Observed Counts and Model Attributes

```{python}
# | label: create-master-long
# | echo: true

# 1. Join Matches to Observed Counts (Inner Join filters to valid stations only)
df_merged = df_matches.merge(df_ccs_2023, on='STATION', how='inner')

# 2. Join to Model Attributes
df_merged = df_merged.merge(
    gdf_model_final[['SEGID', 'FTCLASS', 'ATYPENAME', 'COUNTY_NAME',
                     'SUTRK2022', 'CUTRK2022',
                     'AM_Vol', 'MD_Vol', 'PM_Vol', 'EV_Vol', 'DY_Vol']],
    left_on='MATCHED_SEGID',
    right_on='SEGID',
    how='left'
)
```

### Pivot Periods (Wide -> Long)

```{python}
# 3. Pivot to Long Format (The "Tidy" Data Step)
# We melt the dataframe so "Period" becomes a dimension, not a column name.

# A. Melt Observed Counts
id_vars = [
    "STATION",
    "MATCHED_SEGID",
    "FTCLASS",
    "ATYPENAME",
    "COUNTY_NAME",
    "SUTRK2022",
    "CUTRK2022",
]

df_obs_long = df_merged.melt(
    id_vars=id_vars,
    value_vars=["AVG_AM_VOL", "AVG_MD_VOL", "AVG_PM_VOL", "AVG_EV_VOL"],
    var_name="PERIOD_KEY",
    value_name="OBSERVED_TOTAL",
)

# Extract simple period name (AM, MD, PM, EV, DY) from the column name
df_obs_long["PERIOD"] = df_obs_long["PERIOD_KEY"].str.split("_").str[1]
```

### Convert Modeled Data to Long Format

```{python}
# B. Melt Modeled Counts
# We use the same ID variables to ensure alignment
df_mod_long = df_merged.melt(
    id_vars=id_vars,
    value_vars=["AM_Vol", "MD_Vol", "PM_Vol", "EV_Vol"],
    var_name="MOD_KEY",
    value_name="MODELED_TOTAL",
)

# Extract period name (AM, MD...) - splitting 'AM_Vol' gives 'AM' at index 0
df_mod_long["PERIOD"] = df_mod_long["MOD_KEY"].str.split("_").str[0]
```

### Merge to get Period-level Total Volumes

```{python}
# Merge to get Period-level Total Volumes
df_period = pd.merge(
    df_obs_long.drop(columns=['PERIOD_KEY']),
    df_mod_long.drop(columns=['MOD_KEY']),
    on=id_vars + ['PERIOD'],
    how='inner'
)
```

### Expand Vehicle Types (Vectorized)

```{python}
# First, ensure all percentages exist
df_period["PCT_SUT"] = df_period["SUTRK2022"].fillna(0)
df_period["PCT_CUT"] = df_period["CUTRK2022"].fillna(0)
df_period["PCT_AUTO"] = 1.0 - (df_period["PCT_SUT"] + df_period["PCT_CUT"])

# Melt the percentages into a single column
df_long = df_period.melt(
    id_vars=[
        "STATION",
        "MATCHED_SEGID",
        "FTCLASS",
        "ATYPENAME",
        "COUNTY_NAME",
        "PERIOD",
        "OBSERVED_TOTAL",
        "MODELED_TOTAL",
    ],
    value_vars=["PCT_AUTO", "PCT_SUT", "PCT_CUT"],
    var_name="TYPE_KEY",
    value_name="SHARE",
)

# Clean up Vehicle labels
type_map = {"PCT_AUTO": "Auto", "PCT_SUT": "SUT", "PCT_CUT": "CUT"}
df_long["VEHICLE_TYPE"] = df_long["TYPE_KEY"].map(type_map)

# Single vectorized multiplication for all rows
df_long["OBSERVED"] = df_long["OBSERVED_TOTAL"] * df_long["SHARE"]
df_long["MODELED"] = df_long["MODELED_TOTAL"] * df_long["SHARE"]
```

### Final Cleanup & Export

```{python}
cols_final = [
    "STATION",
    "MATCHED_SEGID",
    "FTCLASS",
    "ATYPENAME",
    "COUNTY_NAME",
    "PERIOD",
    "VEHICLE_TYPE",
    "OBSERVED",
    "MODELED",
]
df_final = df_long[cols_final].copy()

# Calculate stats
df_final["DIFF"] = df_final["MODELED"] - df_final["OBSERVED"]
df_final["PCT_DIFF"] = (df_final["DIFF"] / df_final["OBSERVED"]).round(3)

df_final.to_csv("_output/dashboard_data.csv", index=False)
```

## Interactive Dashboard

```{python}
# | label: transfer-ojs
# | echo: false

# Transfer the clean dataframe to OJS
# ojs_define(data_py=df_final)
```

```{ojs}
//| label: ojs-dashboard
//| echo: false

// 1. IMPORT LIBRARIES
import { aq, op } from '@uwdata/arquero'
import { Plot } from '@observablehq/plot'

// 2. LOAD DATA
data_raw = FileAttachment("_output/dashboard_data.csv").csv({ typed: true })

// 3. DASHBOARD CONTROLS
viewof dashboard_ctrls = Inputs.form({
  group_by: Inputs.select(
    ["FTCLASS", "ATYPENAME", "COUNTY_NAME", "PERIOD", "VEHICLE_TYPE"],
    {label: "Group Variables:", value: "FTCLASS"}
  ),
  filter_county: Inputs.select(
    ["All"].concat(Array.from(new Set(data_raw.map(d => d.COUNTY_NAME))).sort()),
    {label: "Filter County:", value: "All"}
  ),
  filter_atype: Inputs.select(
    ["All"].concat(Array.from(new Set(data_raw.map(d => d.ATYPENAME))).sort()),
    {label: "Filter Area Type:", value: "All"}
  ),
  filter_period: Inputs.select(
    ["All", "AM", "MD", "PM", "EV"],
    {label: "Filter Period:", value: "All"}
  ),
  filter_vehicle: Inputs.select(
    ["All", "Auto", "SUT", "CUT"],
    {label: "Filter Vehicle:", value: "All"}
  )
})

// 4. DATA PROCESSING
dash_data = {
  const { group_by, filter_county, filter_atype, filter_period, filter_vehicle } = dashboard_ctrls;

  let base = aq.from(data_raw)
    .params({
      fc: filter_county, fa: filter_atype, fp: filter_period, fv: filter_vehicle, gb: group_by
    })
    .filter((d, $) => $.fc === "All" || d.COUNTY_NAME === $.fc)
    .filter((d, $) => $.fa === "All" || d.ATYPENAME === $.fa)
    .filter((d, $) => $.fp === "All" || d.PERIOD === $.fp)
    .filter((d, $) => $.fv === "All" || d.VEHICLE_TYPE === $.fv)

  // A. Scatter Data (Station Level)
  let scatter = base
    .groupby("STATION", group_by)
    .rollup({
      Obs_Total: op.sum('OBSERVED'),
      Mod_Total: op.sum('MODELED')
    })
    .derive({
      Label: (d, $) => d[$.gb],
      // Create fields in Thousands for cleaner plotting
      Obs_K: d => d.Obs_Total / 1000,
      Mod_K: d => d.Mod_Total / 1000,
      Pct_Error: d => d.Obs_Total > 0 ? (d.Mod_Total - d.Obs_Total) / d.Obs_Total : 0
    })

  // B. Summary Data (Aggregate Level)
  let summary = base
    .groupby(group_by)
    .rollup({
      Stations: op.count(),
      Obs_Total: op.sum('OBSERVED'),
      Mod_Total: op.sum('MODELED')
    })
    .derive({
      Diff: d => d.Mod_Total - d.Obs_Total,
      Pct_Diff: d => (d.Mod_Total - d.Obs_Total) / d.Obs_Total,
      Label: (d, $) => d[$.gb]
    })
    .orderby(group_by)

  return { scatter, summary }
}

// 5. HELPER: TRENDLINES DATA
// Generates lines for Equal, +/- 10% to 40%
trendlines = {
  const max_k = dash_data.scatter.rollup({ max: op.max("Mod_K") }).get("max") || 1; // Use Mod Max to ensure lines go high enough

  // Slopes: 1.4 = +40%, 0.6 = -40%
  const lines = [
    { slope: 1.4, label: "+40%" },
    { slope: 1.3, label: "+30%" },
    { slope: 1.2, label: "+20%" },
    { slope: 1.1, label: "+10%" },
    { slope: 1.0, label: "Equal" },
    { slope: 0.9, label: "-10%" },
    { slope: 0.8, label: "-20%" },
    { slope: 0.7, label: "-30%" },
    { slope: 0.6, label: "-40%" }
  ];

  // Create 'from-to' points for Plot.line
  return lines.flatMap(l => [
    { x: 0, y: 0, label: l.label, slope: l.slope },
    { x: max_k, y: max_k * l.slope, label: l.label, slope: l.slope }
  ]);
}

// 6. VISUALIZATION

md`### 1. Model vs Observed Volumes (000s)`

Plot.plot({
  height: 500,
  grid: true,
  aspectRatio: 1,
  x: { label: "Observed Volume (000s)" },
  y: { label: "Modeled Volume (000s)" },
  style: { backgroundColor: 'transparent' },
  color: { legend: true, label: dashboard_ctrls.group_by },
  marks: [
    // Trendlines (Adaptive Color)
    Plot.line(trendlines, {
      x: "x", y: "y", z: "slope",
      stroke: "currentColor", strokeOpacity: 0.2, strokeDasharray: "4"
    }),
    Plot.text(trendlines.filter(d => d.x > 0), {
      x: "x", y: "y", text: "label",
      dx: 5, fill: "currentColor", fillOpacity: 0.5, textAnchor: "start"
    }),

    // Equal Line (Stronger)
    Plot.line(trendlines.filter(d => d.slope === 1.0), {
      x: "x", y: "y", stroke: "currentColor", strokeOpacity: 0.5, strokeWidth: 1.5
    }),

    // Data Dots
    Plot.dot(dash_data.scatter, {
      x: "Obs_K",
      y: "Mod_K",
      fill: "Label",
      stroke: "white", strokeOpacity: 0.5, r: 4,
      title: d => `Station: ${d.STATION}\nGroup: ${d.Label}\nObs: ${d.Obs_Total.toLocaleString()}\nMod: ${d.Mod_Total.toLocaleString()}\nError: ${(d.Pct_Error*100).toFixed(1)}%`
    })
  ]
})

md`### 2. Model vs Observed Percent Error`

Plot.plot({
  height: 400,
  grid: true,
  x: { label: "Observed Volume (000s)" },
  y: { label: "Percent Error", tickFormat: "+%", domain: [-1, 2] }, // Cap visual Y axis for better readability
  style: { backgroundColor: 'transparent' },
  color: { legend: true, label: dashboard_ctrls.group_by },
  marks: [
    // Zero Error Line (Adaptive)
    Plot.ruleY([0], { stroke: "currentColor", strokeWidth: 1.5, strokeOpacity: 0.5 }),

    // Error Dots
    Plot.dot(dash_data.scatter, {
      x: "Obs_K",
      y: "Pct_Error",
      fill: "Label",
      stroke: "white", strokeOpacity: 0.5, r: 4,
      title: d => `Station: ${d.STATION}\nObs: ${d.Obs_Total.toLocaleString()}\nError: ${(d.Pct_Error*100).toFixed(1)}%`
    })
  ]
})

md`### 3. Aggregate Comparison & Statistics`

// Prepare Bar Data
bar_data = dash_data.summary.fold(["Obs_Total", "Mod_Total"], { as: ["Metric", "Volume"] })
    .derive({ Metric_Label: d => d.Metric === "Obs_Total" ? "Observed" : "Modeled" })

Plot.plot({
  marginLeft: 150,
  height: Math.max(300, dash_data.summary.numRows() * 40),
  color: {
     domain: ["Observed", "Modeled"],
     range: ["#bababa", "#1976d2"],
     legend: true
  },
  style: { backgroundColor: 'transparent' },
  x: { label: "Total Volume", grid: true, tickFormat: "s" },
  y: { axis: null },
  fy: { label: null, axis: "left" },
  marks: [
    Plot.barX(bar_data, {
      x: "Volume", y: "Metric_Label", fy: "Label",
      fill: "Metric_Label",
      title: d => `${d.Metric_Label}: ${d.Volume.toLocaleString()}`,
      sort: { fy: "x", reduce: "max", order: "descending" }
    }),
    Plot.text(bar_data, {
      x: "Volume", y: "Metric_Label", fy: "Label",
      text: d => d.Volume.toLocaleString(),
      dx: 5, textAnchor: "start", fill: "currentColor", fontSize: 10
    })
  ]
})

Inputs.table(dash_data.summary, {
  columns: ["Label", "Stations", "Obs_Total", "Mod_Total", "Diff", "Pct_Diff"],
  header: {
    Label: dashboard_ctrls.group_by,
    Obs_Total: "Observed",
    Mod_Total: "Modeled",
    Diff: "Diff",
    Pct_Diff: "% Error"
  },
  format: {
    Obs_Total: d => d.toLocaleString(),
    Mod_Total: d => d.toLocaleString(),
    Diff: d => d.toLocaleString(),
    Pct_Diff: d => (d * 100).toFixed(1) + "%"
  }
})
```
